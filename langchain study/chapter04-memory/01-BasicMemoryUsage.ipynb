{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1、ChatMessageHistory的使用\n",
    "场景1: 记忆存储"
   ],
   "id": "c5b0fee1a3a56d82"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ChatMessageHistory\n",
    "\n",
    "# 1、ChatMessageHistory的实例化\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# 2、添加相关的消息进行存储\n",
    "history.add_user_message(\"你好\")\n",
    "history.add_ai_message(\"很高兴认识你\")\n",
    "\n",
    "# 3、打印存储的消息\n",
    "print(history.messages)"
   ],
   "id": "6b14fcebf1860b00",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "场景2: 对接LLM",
   "id": "4c571b917f066199"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n"
   ],
   "id": "89f3a5e4a5c79486",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ChatMessageHistory # Assuming this is imported\n",
    "\n",
    "# 1、ChatMessageHistory的实例化\n",
    "history = ChatMessageHistory()\n",
    "\n",
    "# 2、添加相关的消息进行存储\n",
    "history.add_user_message(\"你好\")\n",
    "history.add_ai_message(\"很高兴认识你\")\n",
    "history.add_user_message(\"帮我计算1 + 2 * 3 = ?\")\n",
    "\n",
    "# Assuming 'llm' is an instantiated Language Model (e.g., from ChatOpenAI)\n",
    "response = chat_model.invoke(history.messages)\n",
    "print(response.content)"
   ],
   "id": "bc6ade85f7e1eb3d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2、ConversationBufferMemory的使用\n",
    "\n",
    "举例1: 返回存储的字符串信息"
   ],
   "id": "d0cd866a3351ea92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1、ConversationBufferMemory的实例化\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 2、存储相关的消息\n",
    "memory.save_context(inputs={\"input\": \"你好, 我叫小明\"}, outputs={\"output\": \"很高兴认识你\"})\n",
    "memory.save_context(inputs={\"input\": \"帮我回答一下1+2*3=?\"}, outputs={\"output\": \"7\"})\n",
    "\n",
    "# 3、获取存储的信息\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "5ae5b8b48431fc2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2: 以消息列表的方式返回存储的信息",
   "id": "90de9f0d6e62d3e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# 1、ConversationBufferMemory的实例化\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 2、存储相关的消息\n",
    "# inputs对应的是用户消息, outputs对应的是ai消息\n",
    "memory.save_context(inputs={\"human\":\"你好, 我叫小明\"}, outputs={\"ai\":\"很高兴认识你\"})\n",
    "memory.save_context(inputs={\"input\":\"帮我回答一下1+2*3=?\"}, outputs={\"output\":\"7\"})\n",
    "\n",
    "# 3、获取存储的信息\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "# 说明: 返回的字典结构的key叫history.\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# 返回消息列表的方式2:\n",
    "print(memory.chat_memory.messages)\n",
    "\n",
    "# 说明: 返回的字典结构的key叫history."
   ],
   "id": "48a957ce56b3b918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例3: 结合大模型、提示词模板的使用 (PromptTemplate)",
   "id": "9dd24710ebd6e02f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # Assuming PromptTemplate is imported\n",
    "from langchain.memory import ConversationBufferMemory # Assuming ConversationBufferMemory is imported\n",
    "from langchain.chains import LLMChain # Assuming LLMChain is imported\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template = \"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话历史: {history}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 3、提供memory实例\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "# 4、提供Chain\n",
    "# Assuming 'llm' is an instantiated language model (e.g., ChatOpenAI)\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "\n",
    "# # 4、提供Chain\n",
    "# Assuming LLMChain, llm, prompt_template, and memory are defined from the previous snippet\n",
    "# chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"question\":\"你好, 我的名字叫小明\"})\n",
    "print(response)"
   ],
   "id": "945139784108fe2a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.invoke({\"question\":\"我的名字是什么\"})\n",
    "print(response)"
   ],
   "id": "92c94626b44536fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例4: 基于举例3, 显式的设置memory的key的值",
   "id": "e6591e9519a6f389"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # Assuming PromptTemplate is imported\n",
    "from langchain.memory import ConversationBufferMemory # Assuming ConversationBufferMemory is imported\n",
    "from langchain.chains import LLMChain # Assuming LLMChain is imported\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template = \"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话历史: {chatHistory}\n",
    "\n",
    "人类问题: {question}\n",
    "\n",
    "回复:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# 3、提供memory实例\n",
    "memory = ConversationBufferMemory(memory_key = \"chatHistory\")\n",
    "# 更改变量值\n",
    "\n",
    "# 4、提供Chain\n",
    "# Assuming 'llm' is an instantiated language model (e.g., ChatOpenAI)\n",
    "chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "\n",
    "# # 4、提供Chain\n",
    "# Assuming LLMChain, llm, prompt_template, and memory are defined from the previous snippet\n",
    "# chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"question\":\"你好, 我的名字叫小明\"})\n",
    "print(response)"
   ],
   "id": "277e0cb3dcaf0910",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例5: 结合大模型、提示词模板的使用 (ChatPromptTemplate)",
   "id": "b4733a36eb4c681b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage # Langchain_core instead of langchain.core.messages\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import MessagesPlaceholder, ChatPromptTemplate, HumanMessagePromptTemplate # Langchain_core instead of langchain.core.prompts\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. 创建LLM\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini')\n",
    "\n",
    "# 3. 创建Prompt\n",
    "# Note: The original image uses (\"system\", \"...\", MessagesPlaceholder, (\"human\", \"...\"))\n",
    "# ChatPromptTemplate.from_messages handles this structure automatically.\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是一个与人类对话的机器人。\"),\n",
    "    MessagesPlaceholder(variable_name='history'),\n",
    "    (\"human\", \"问题: {question}\")\n",
    "])\n",
    "\n",
    "# 4. 创建Memory\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "# 5. 创建LLMChain\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm, memory=memory)\n",
    "\n",
    "# 6. 调用LLMChain\n",
    "\n",
    "res1 = llm_chain.invoke({\"question\": \"中国首都都在哪里?\"})\n",
    "print(res1, end=\"\\n\\n\")\n",
    "\n",
    "res2 = llm_chain.invoke({\"question\": \"我刚刚问了什么\"})\n",
    "print(res2)"
   ],
   "id": "cf6952e006a80459",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3、ConversationChain的使用\n",
    "举例1: 以PromptTemplate为例"
   ],
   "id": "3e9258b8c2cf6a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # Assuming PromptTemplate is imported\n",
    "from langchain.memory import ConversationBufferMemory # Assuming ConversationBufferMemory is imported\n",
    "from langchain.chains import LLMChain # Assuming LLMChain is imported\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    template = \"\"\"\n",
    "你可以与人类对话。\n",
    "\n",
    "当前对话历史: {history}\n",
    "\n",
    "人类问题: {input}\n",
    "\n",
    "回复:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# # 3、提供memory实例\n",
    "# memory = ConversationBufferMemory(memory_key = \"chatHistory\")\n",
    "# # 更改变量值\n",
    "#\n",
    "# # 4、提供Chain\n",
    "# # Assuming 'llm' is an instantiated language model (e.g., ChatOpenAI)\n",
    "# chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "# 3.创建ConversationChain的实列\n",
    "chain = ConversationChain(llm=chat_model, prompt=prompt_template)\n",
    "\n",
    "# # 4、提供Chain\n",
    "# Assuming LLMChain, llm, prompt_template, and memory are defined from the previous snippet\n",
    "# chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"input\":\"你好, 我的名字叫小明\"})\n",
    "print(response)"
   ],
   "id": "5257ec5b68792545",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "response = chain.invoke({\"input\", \"我的名字叫什么\"})\n",
    "print(response)"
   ],
   "id": "6e19d535a3a0d21d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2: 使用默认提供的提示词模板",
   "id": "fbe217eb3b16148c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains.conversation.base import ConversationChain\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate # Assuming PromptTemplate is imported\n",
    "from langchain.memory import ConversationBufferMemory # Assuming ConversationBufferMemory is imported\n",
    "from langchain.chains import LLMChain # Assuming LLMChain is imported\n",
    "\n",
    "# prompt_template = PromptTemplate.from_template(\n",
    "#     template = \"\"\"\n",
    "# 你可以与人类对话。\n",
    "#\n",
    "# 当前对话历史: {history}\n",
    "#\n",
    "# 人类问题: {input}\n",
    "#\n",
    "# 回复:\n",
    "# \"\"\"\n",
    "# )\n",
    "\n",
    "# # 3、提供memory实例\n",
    "# memory = ConversationBufferMemory(memory_key = \"chatHistory\")\n",
    "# # 更改变量值\n",
    "#\n",
    "# # 4、提供Chain\n",
    "# # Assuming 'llm' is an instantiated language model (e.g., ChatOpenAI)\n",
    "# chain = LLMChain(llm=chat_model, prompt=prompt_template, memory=memory)\n",
    "\n",
    "# 3.创建ConversationChain的实列\n",
    "chain = ConversationChain(llm=chat_model)\n",
    "\n",
    "# # 4、提供Chain\n",
    "# Assuming LLMChain, llm, prompt_template, and memory are defined from the previous snippet\n",
    "# chain = LLMChain(llm=llm, prompt=prompt_template, memory=memory)\n",
    "\n",
    "response = chain.invoke({\"input\":\"你好, 我的名字叫小明\"})\n",
    "print(response)"
   ],
   "id": "594eeff38c79adc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4、ConversationBufferWindowMemory的使用\n",
    "举例1:"
   ],
   "id": "8797c188d7c8bc31"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 2. 实例化ConversationBufferWindowMemory对象, 设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=2)\n",
    "\n",
    "# 3. 保存消息\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你是谁\"}, {\"output\": \"我是AI助手\"})\n",
    "memory.save_context({\"input\": \"你的生日是哪天?\"}, {\"output\": \"我不清楚\"})\n",
    "\n",
    "# 4. 读取内存中消息 (返回消息内容的纯文本)\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "bc9582774c57aa1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2: 返回消息构成的上下文记忆",
   "id": "c32826b629a82ae0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# 2. 实例化ConversationBufferWindowMemory对象, 设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=2, return_messages=True)\n",
    "\n",
    "# 3. 保存消息\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你是谁\"}, {\"output\": \"我是AI助手小智\"})\n",
    "memory.save_context({\"input\": \"初次对话, 你能介绍一下你自己吗?\"}, {\"output\": \"当然可以了。我是一个无所不能的小智。\"})\n",
    "\n",
    "# 4. 读取内存中消息 (返回消息内容的纯文本)\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "44c1911546e2a890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例3: 结合LLM、chain的使用",
   "id": "c69384dfda96f30c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "# 1.导入相关包\n",
    "from langchain_core.prompts.prompt import PromptTemplate\n",
    "from langchain.chains.llm import LLMChain\n",
    "# 2.定义模版\n",
    "template = \"\"\"以下是人类与AI之间的友好对话描述。AI表现得很健谈，并提供了大量来自其上下文的\n",
    "具体细节。如果AI不知道问题的答案，它会表示不知道。\n",
    "当前对话：\n",
    "{history}\n",
    "Human: {question}\n",
    "AI:\"\"\"\n",
    "# 3.定义提示词模版\n",
    "prompt_template = PromptTemplate.from_template(template)\n",
    "# 4.创建大模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 5.实例化ConversationBufferWindowMemory对象，设定窗口阈值\n",
    "memory = ConversationBufferWindowMemory(k=1)\n",
    "# 6.定义LLMChain\n",
    "conversation_with_summary = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt_template,\n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    ") #\n",
    "# 7.执行链（第一次提问）\n",
    "respon1 = conversation_with_summary.invoke({\"question\":\"你好，我是孙小空\"})\n",
    "# print(respon1)\n",
    "# 8.执行链（第二次提问）\n",
    "respon2 =conversation_with_summary.invoke({\"question\":\"我还有两个师弟，一个是猪小戒，一个是沙小僧\"})\n",
    "# print(respon2)\n",
    "# 9.执行链（第三次提问）\n",
    "respon3 =conversation_with_summary.invoke({\"question\":\"我今年高考，竟然考上了1本\"})\n",
    "# print(respon3)\n",
    "# 10.执行链（第四次提问）\n",
    "respon4 =conversation_with_summary.invoke({\"question\":\"我叫什么？\"})\n",
    "print(respon4)"
   ],
   "id": "e23d20cf4bf13c19",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
