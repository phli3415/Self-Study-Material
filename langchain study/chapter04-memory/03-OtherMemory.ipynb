{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1、ConversationTokenBufferMemory的使用\n",
    "\n",
    "举例1:"
   ],
   "id": "eac5c31e1573b528"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. 创建大模型\n",
    "# Note: For this example to work correctly, the LLM must be available and configured.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. 定义ConversationTokenBufferMemory对象\n",
    "#    llm: 用于计算token数\n",
    "#    max_token_limit: 设置token上限\n",
    "memory = ConversationTokenBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=10  # 设置token上限，默认2000 (This value is too small for a real conversation, just for demonstration)\n",
    ")\n",
    "\n",
    "# 添加对话\n",
    "memory.save_context({\"input\": \"你好吗?\"}, {\"output\": \"我很好, 谢谢!\"})\n",
    "memory.save_context({\"input\": \"今天天气如何?\"}, {\"output\": \"晴天, 25度\"})\n",
    "\n",
    "# 查看当前记忆\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "14e1735887945599",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2、ConversationSummaryMemory的使用\n",
    "\n",
    "举例1: 如果实例化`ConversationSummaryMemory`前，没有历史消息，可以使用构造方法实例化"
   ],
   "id": "84bba431a1e8b6c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. 创建大模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. 定义ConversationSummaryMemory对象\n",
    "memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "# 4. 存储消息\n",
    "memory.save_context({\"input\": \"你好\"}, {\"output\": \"怎么了\"})\n",
    "memory.save_context({\"input\": \"你是谁\"}, {\"output\": \"我是AI助手小智\"})\n",
    "memory.save_context({\"input\": \"初次对话, 你能介绍一下你自己吗?\"}, {\"output\": \"当然可以了。我是一个无所不能的小智。\"})\n",
    "\n",
    "# 5. 读取消息 (总结后的)\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "96b929d0ef9cc64d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2: 如果实例化`ConversationSummaryMemory`前, 已经有历史消息, 可以调用`from_messages()`实例化",
   "id": "28189001ea8250f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 2. 定义ChatMessageHistory对象 (Note: llm is created here, not ChatMessageHistory)\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 3. 假设原始消息\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"你好, 你是谁?\")\n",
    "history.add_ai_message(\"我是AI助手小智\")\n",
    "\n",
    "# 4. 创建ConversationSummaryMemory的实例\n",
    "memory = ConversationSummaryMemory.from_messages(\n",
    "    llm=llm,\n",
    "    #是生成摘要的原材料，保留完整对话供必要时溯洄，当新增对话时，LLM需要结合原始历史生成新摘要\n",
    "    chat_memory=history,\n",
    ")\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "memory.save_context(inputs={\"human\":\"我的名字叫小明\"}, outputs={\"ai\": \"很高兴认识你\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "print(memory.chat_memory.messages)"
   ],
   "id": "13330f3ed18bc65e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3、ConversationSummaryBufferMemory的使用\n",
    "\n",
    "举例1:"
   ],
   "id": "f27a5b48fd826e57"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "# 获取大型模型\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# 实例化ConversationSummaryBufferMemory\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm = llm,\n",
    "    max_token_limit=40, # 控制缓冲器区的大小\n",
    "    return_messages=True,\n",
    ")\n",
    "\n",
    "# 向memory中存储信息\n",
    "memory.save_context(inputs={\"input\":\"你好, 我的名字叫小明\"}, outputs={\"output\":\"很高兴认识你\"})\n",
    "memory.save_context(inputs={\"input\":\"李白是哪个朝代的诗人\"}, outputs={\"output\":\"李白是唐朝的诗人\"})\n",
    "memory.save_context(inputs={\"input\":\"唐宋八大家里有苏轼吗？\"}, outputs={\"output\":\"有\"})\n",
    "\n",
    "print(memory.load_memory_variables({}))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(memory.chat_memory.messages)"
   ],
   "id": "e8d1b80d623bcb5e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.chains.llm import LLMChain\n",
    "# 1、初始化大语言模型\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    max_tokens=500\n",
    ")\n",
    "#2、定义提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是电商客服助手，用中文友好回复用户问题。保持专业但亲切的语气。\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 3、创建带摘要缓冲的记忆系统\n",
    "memory = ConversationSummaryBufferMemory(\n",
    "    llm=llm,\n",
    "    max_token_limit=400,\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")\n",
    "#4、创建对话链\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    memory=memory,\n",
    ")\n",
    "#5、模拟多轮对话\n",
    "dialogue = [\n",
    "    (\"你好，我想查询订单12345的状态\", None),\n",
    "    (\"这个订单是上周五下的\", None),\n",
    "    (\"我现在急着用，能加急处理吗\", None),\n",
    "    (\"等等，我可能记错订单号了，应该是12346\", None),\n",
    "    (\"对了，你们退货政策是怎样的\", None)\n",
    "]\n",
    "#6、执行对话\n",
    "for user_input, _ in dialogue:\n",
    "    response = chain.invoke({\"input\": user_input})\n",
    "    print(f\"用户: {user_input}\")\n",
    "    print(f\"客服: {response['text']}\\n\")\n",
    "\n",
    "# 7、查看当前记忆状态\n",
    "print(\"\\n=== 当前记忆内容 ===\")\n",
    "print(memory.load_memory_variables({}))"
   ],
   "id": "53610dad21f1e6a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 4、ConversationEntityMemory的使用（了解）",
   "id": "e32f3163ccc861f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains.conversation.base import LLMChain\n",
    "from langchain.memory import ConversationEntityMemory\n",
    "from langchain.memory.prompt import ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 初始化大语言模型\n",
    "llm = ChatOpenAI(model_name='gpt-4o-mini', temperature=0)\n",
    "# 使用LangChain为实体记忆设计的预定义模板\n",
    "prompt = ENTITY_MEMORY_CONVERSATION_TEMPLATE\n",
    "# 初始化实体记忆\n",
    "memory = ConversationEntityMemory(llm=llm)\n",
    "# 提供对话链\n",
    "chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=ENTITY_MEMORY_CONVERSATION_TEMPLATE,\n",
    "    memory=ConversationEntityMemory(llm=llm),\n",
    "#verbose=True, # 设置为True可以看到链的详细推理过程\n",
    ")\n",
    "#进行几轮对话，记忆组件会在后台自动提取和存储实体信息\n",
    "chain.invoke(input=\"你好，我叫蜘蛛侠。我的好朋友包括钢铁侠、美国队长和绿巨人。\")\n",
    "chain.invoke(input=\"我住在纽约。\")\n",
    "chain.invoke(input=\"我使用的装备是由斯塔克工业提供的。\")\n",
    "# 查询记忆体中存储的实体信息\n",
    "print(\"\\n当前存储的实体信息:\")\n",
    "print(chain.memory.entity_store.store)\n",
    "# 基于记忆进行提问\n",
    "answer = chain.invoke(input=\"你能告诉我蜘蛛侠住在哪里以及他的好朋友有哪些吗？\")\n",
    "print(\"\\nAI的回答:\")\n",
    "print(answer)"
   ],
   "id": "c354f43ca3a17a26",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3.5 ConversationKGMemory(了解)",
   "id": "3e897edd36a00476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. 导入相关包\n",
    "from langchain.memory import ConversationKGMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# 2. 定义LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 3. 定义ConversationKGMemory对象\n",
    "memory = ConversationKGMemory(llm=llm)\n",
    "\n",
    "# 4. 保存会话\n",
    "memory.save_context({\"input\": \"向山姆问好\"}, {\"output\": \"山姆是谁\"})\n",
    "memory.save_context({\"input\": \"山姆是我的朋友\"}, {\"output\": \"好的\"})\n",
    "\n",
    "# 5. 查询会话\n",
    "memory.load_memory_variables({\"input\": \"山姆是谁\"})"
   ],
   "id": "d1393a10c70c518d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
