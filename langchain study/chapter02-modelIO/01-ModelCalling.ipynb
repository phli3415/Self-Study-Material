{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Model Invocation Classification\n",
    "The image outlines two different angles for classifying how models are invoked (called):\n",
    "\n",
    "1. 角度1: 按照模型功能的不同 (Angle 1: By Model Function)\n",
    "This categorizes models based on their primary function or type:\n",
    "\n",
    "非对话模型 (Non-Conversational Model): (LLMs, Text Model)\n",
    "\n",
    "These models are typically designed for single-turn tasks like generating a specific piece of text, summarization, or translation.\n",
    "\n",
    "对话模型 (Conversational Model): (Chat Models) (推荐)\n",
    "\n",
    "These models are optimized for multi-turn conversations, maintaining context, and simulating human-like dialogue.\n",
    "\n",
    "嵌入模型 (Embedding Model): (Embedding Models)\n",
    "\n",
    "These models convert text into numerical vectors (embeddings) for tasks like semantic search, clustering, or similarity comparison.\n",
    "\n",
    "2. 角度2: 按照模型调用时, 参数书写的位置的不同 (Angle 2: By Location of Parameters During Model Invocation)\n",
    "This classifies how key configuration parameters like api-key, base_url, and model-name are provided to the model during its call:\n",
    "\n",
    "硬编码的方式 (Hardcoding Method):\n",
    "\n",
    "将参数书写在代码中 (Writing the parameters directly in the code).\n",
    "\n",
    "使用环境变量的方式 (Using Environment Variables Method):\n",
    "\n",
    "Storing parameters as system environment variables, which the code then reads. This is generally preferred for security (especially for API keys).\n",
    "\n",
    "使用配置文件的方式 (Using Configuration File Method) (推荐):\n",
    "\n",
    "Storing parameters in a separate file (e.g., JSON, YAML, or INI), which the application loads at runtime.\n",
    "\n",
    "\n",
    "3. 角度3: 具体 API 的调用 (Angle 3: Specific API Invocation)\n",
    "This angle classifies model calls based on the specific API layer or framework being used:\n",
    "\n",
    "使用 LangChain 提供的 API (推荐) (Using the API provided by LangChain - Recommended)\n",
    "\n",
    "This approach uses LangChain as an abstraction layer. LangChain's components (like LLMs, Chat Models, and Embeddings) provide a unified interface, allowing developers to easily switch between different model providers (e.g., OpenAI, Google, Anthropic) without rewriting their core application logic.\n",
    "\n",
    "使用 OpenAI 官方的 API (Using the official OpenAI API)\n",
    "\n",
    "This is a direct call to the API provided by the model provider (in this case, OpenAI). It offers the most direct access to the model's features but ties the code directly to that provider's specific API syntax.\n",
    "\n",
    "使用其它平台提供的 API (Using APIs provided by other platforms)\n",
    "\n",
    "This refers to directly calling APIs from other model providers, such as Google (e.g., Gemini API), Anthropic (Claude API), or various open-source model hosting platforms.\n"
   ],
   "id": "498a2f1c049ec894"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Non-Conversational Model Invocation Code",
   "id": "ae18d13dd1cabb09"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAI\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# ##########核心代码############\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"), model=\"gpt-3.5-turbo-instruct\")\n",
    "string = llm.invoke(\"写一首关于春天的诗\") # 直接输入字符串\n",
    "print(string)"
   ],
   "id": "b1c9b55306267b48",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "os.environ.get(\"OPENAI_API_KEY\")\n"
   ],
   "id": "ae9127d4305ab722",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#Chat Models(对话模型)\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "#os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ[\"OPENAI_BASE_URL\"] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "########核心代码############\n",
    "chat_model = ChatOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"), model=\"gpt-4o-mini\")\n",
    "messages = [ SystemMessage(content=\"我是人工智能助手，我叫小智\"), HumanMessage(content=\"你好，我是小明，很高兴认识你\")]\n",
    "response = chat_model.invoke(messages) # 输入消息列表\n",
    "print(type(response)) # <class 'langchain_core.messages.ai.AIMessage'>\n",
    "print(response.content)"
   ],
   "id": "a3486626fdd7638f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embedding Model\n",
    "\n",
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "dotenv.load_dotenv()\n",
    "# os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "# os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "#############\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "res1 = embeddings_model.embed_query('我是文档中的数据')\n",
    "print(res1)\n",
    "# 打印结果：[-0.004306625574827194, 0.003083756659179926, -0.013916781172156334, ...., ]"
   ],
   "id": "3a20e0a54e4cd456",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3 角度2出发：参数位置不同举例",
   "id": "b90a9ef5c29ecbc9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 硬编码狗都不用\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 调用非对话模型:\n",
    "# llm = OpenAI(...)\n",
    "\n",
    "# 调用对话模型:\n",
    "chat_model = ChatOpenAI(\n",
    "    # 必须设置的3个参数\n",
    "    model_name=\"gpt-4o-mini\", # 默认使用的是 gpt-3.5-turbo 模型\n",
    "    base_url=\"https://api.openai-proxy.org/v1\",\n",
    "    api_key=\"sk-use your own key\",\n",
    ")\n",
    "\n",
    "# 调用模型\n",
    "response = chat_model.invoke(\"什么是langchain?\")\n",
    "\n",
    "# 查看响应的文本\n",
    "print(response.content)"
   ],
   "id": "dc5fa12f8bd48ca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 从环境变量读取密钥\n",
    "# 环境变量放到run-> edit configuration里\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"], # 动态获取\n",
    "    base_url=os.environ[\"OPENAI_BASE_URL\"],\n",
    "    model=\"gpt-4o-mini\",\n",
    ")\n",
    "response = llm.invoke(\"LangChain 是什么？\")\n",
    "print(response.content)"
   ],
   "id": "8e28dfb97db3b2d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.3 使用配置文件的方式 (Method of Using Configuration Files)",
   "id": "93f15595b7e925d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 加载配置文件\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 1、获取对话模型:\n",
    "chat_model = ChatOpenAI(\n",
    "    # 必须设置的3个参数\n",
    "    model_name=\"gpt-4o-mini\", # 默认使用的是 gpt-3.5-turbo 模型\n",
    "    # base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "# 2、调用模型\n",
    "response = chat_model.invoke(\"什么是langchain?\")\n",
    "\n",
    "# 3、查看响应的文本\n",
    "# print(response.content)\n",
    "print(response)"
   ],
   "id": "595536dc29bf9698",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "调用方法2",
   "id": "377954b0f2c6cc7b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "# 加载配置文件\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]= os.getenv(\"OPENAI_API_KEY\")\n",
    "# 1、获取对话模型:\n",
    "#\n",
    "chat_model = ChatOpenAI(\n",
    "    # 必须设置的3个参数\n",
    "    model_name=\"gpt-4o-mini\", # 默认使用的是 gpt-3.5-turbo 模型\n",
    "    # base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    # api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    # 当没有传入base_url 和 api_key, 默认会从环境变量里去取\n",
    ")\n",
    "\n",
    "# 2、调用模型\n",
    "response = chat_model.invoke(\"什么是langchain?\")\n",
    "\n",
    "# 3、查看响应的文本\n",
    "# print(response.content)\n",
    "print(response)"
   ],
   "id": "2831b9c1d796ae8a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "51338fcda5552808",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
