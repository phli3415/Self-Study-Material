{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3.2 关于多轮对话与上下文记忆",
   "id": "eeeda6098c3bd3fb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    max_tokens = 1024\n",
    ")\n"
   ],
   "id": "e72bf359317a1bec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "大模型没有上下文记忆能力",
   "id": "3fe3dfbdf077247a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "sys_message = SystemMessage(\n",
    "    content=\"我是一个人工智能的助手，我的名字叫小智\",\n",
    ")\n",
    "human_message = HumanMessage(content=\"猫王是一只猫吗？\")\n",
    "messages = [sys_message, human_message]\n",
    "#调用大模型，传入messages\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)\n",
    "\n",
    "response1 = chat_model.invoke(\"你叫什么名字？\")\n",
    "print(response1.content)\n",
    "# 不会记得自己叫小智"
   ],
   "id": "516f86794f9fb04c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "sys_message = SystemMessage(\n",
    "content=\"我是一个人工智能的助手，我的名字叫小智\",\n",
    ")\n",
    "\n",
    "human_message = HumanMessage(content=\"猫王是一只猫吗？\")\n",
    "human_message1 = HumanMessage(content=\"你叫什么名字？\")\n",
    "# ai只会回答最后一个问题\n",
    "# 前面一大窜都是作为过程存在，ai只关心最后一个HumanMessage\n",
    "messages = [sys_message, human_message,human_message1]\n",
    "#调用大模型，传入messages\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)"
   ],
   "id": "2c53f4f825ba5b1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "\n",
    "sys_message = SystemMessage(\n",
    "    content=\"我是一个人工智能的助手, 我的名字叫小智\",\n",
    ")\n",
    "human_message = HumanMessage(content=\"猫是一只猫吗？\")\n",
    "\n",
    "sys_message1 = SystemMessage(\n",
    "    content=\"我可以做很多事情, 有需要就找我吧\",\n",
    ")\n",
    "\n",
    "human_message1 = HumanMessage(content=\"你叫什么名字？\")\n",
    "\n",
    "messages = [sys_message, human_message, sys_message1, human_message1]\n",
    "\n",
    "# 调用大模型, 传入messages\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)\n",
    "# 回复：我叫小智。很高兴认识你！有什么我可以帮助你的吗？"
   ],
   "id": "dfeb4251f9b3d283",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "# 第1组\n",
    "sys_message = SystemMessage(\n",
    "    content=\"我是一个人工智能的助手，我的名字叫小智\"\n",
    ")\n",
    "human_message = HumanMessage(content=\"猫王是一只猫吗？\")\n",
    "messages = [sys_message, human_message]\n",
    "# 第2组\n",
    "sys_message1 = SystemMessage(\n",
    "content=\"我可以做很多事情，有需要就找我吧\",\n",
    ")\n",
    "human_message1 = HumanMessage(content=\"你叫什么名字？\")\n",
    "messages1 = [sys_message1,human_message1]\n",
    "#调用大模型，传入messages\n",
    "response = chat_model.invoke(messages)\n",
    "print(response.content)\n",
    "response = chat_model.invoke(messages1)\n",
    "print(response.content)\n",
    "# 说不出来"
   ],
   "id": "6f74d16e80ec6e0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"我是一个人工智能助手, 我的名字叫小智\"),\n",
    "    HumanMessage(content=\"人工智能英语怎么说？\"),\n",
    "    AIMessage(content=\"AI\"),\n",
    "    HumanMessage(content=\"你叫什么名字\"),\n",
    "]\n",
    "\n",
    "messages1 = [\n",
    "    SystemMessage(content=\"我是一个人工智能助手, 我的名字叫小智\"),\n",
    "    HumanMessage(content=\"很高兴认识你\"),\n",
    "    AIMessage(content=\"我也很高兴认识你\"),\n",
    "    HumanMessage(content=\"你叫什么名字？\"),\n",
    "]\n",
    "\n",
    "messages2 = [\n",
    "    SystemMessage(content=\"我是一个人工智能助手, 我的名字叫小智\"),\n",
    "    HumanMessage(content=\"人工智能英语怎么说？\"),\n",
    "    AIMessage(content=\"AI\"),\n",
    "    HumanMessage(content=\"你叫什么名字？\"),\n",
    "]\n",
    "\n",
    "response = chat_model.invoke(messages2)"
   ],
   "id": "d1c24be8bc8c701",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "3. 关于模型调用的方法的说明\n",
    "invoke() / stream()\n",
    "batch(): 批量的调用 (Batch invocation)\n",
    "ainvoke() / astream() / abatch(): 异步方法的调用 (Asynchronous method invocation)\n",
    "\n",
    "举例1: 阻塞时"
   ],
   "id": "695cb7276edd3f17"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY1\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")#初始化大模型\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "# 创建消息\n",
    "messages = [HumanMessage(content=\"你好，请介绍一下自己\")]\n",
    "# 非流式调用LLM获取响应\n",
    "response = chat_model.invoke(messages)\n",
    "# 打印响应内容\n",
    "print(response)"
   ],
   "id": "d2e1bd9fde5bb70",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例2：流式调用",
   "id": "c3008c036ab5e112"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "# 从 langchain_core.messages 导入 HumanMessage\n",
    "from langchain_core.messages import HumanMessage\n",
    "# 从 langchain_openai 导入 ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "# 导入 dotenv 库来加载 .env 文件中的环境变量\n",
    "import dotenv\n",
    "\n",
    "# 加载环境变量 (例如从 .env 文件中加载 OPENAI_API_KEY 和 OPENAI_BASE_URL)\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 设置环境变量，确保模型能够找到 API Key 和 Base URL\n",
    "# 这里的代码假设您在 .env 文件中设置了这些变量\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# 如果您使用的是非官方的 API 代理，才需要设置 BASE_URL\n",
    "# os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "# 初始化大模型\n",
    "# streaming=True 表示启用流式输出，这是 stream() 方法的关键\n",
    "chat_model = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    streaming=True  # 启用流式输出\n",
    ")\n",
    "\n",
    "# 创建消息列表\n",
    "messages = [HumanMessage(content=\"你好，请介绍一下自己\")]\n",
    "\n",
    "# 调用 LLM 并流式响应\n",
    "print(\"\\n[开始流式输出]:\")\n",
    "\n",
    "# 循环处理 chat_model.stream(messages) 返回的每个块 (chunk)\n",
    "# stream() 方法会生成响应片段\n",
    "for chunk in chat_model.stream(messages):\n",
    "    # 逐个打印内容片段\n",
    "    # end=\"\" 确保每次打印后不换行\n",
    "    # flush=True 强制刷新输出缓冲区，确保内容立即显示，实现实时流式效果\n",
    "    print(chunk.content, end=\"\", flush=True)\n",
    "\n",
    "# 打印换行符以结束输出\n",
    "print(\"\\n[流式输出结束]\")"
   ],
   "id": "d0dc39eb937e14c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例3：使用batch测试批量调用",
   "id": "73640923a54d82d3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "dotenv.load_dotenv()\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY\")\n",
    "# os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "# 初始化大模型\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "messages1 = [SystemMessage(content=\"你是一位乐于助人的智能小助手\"),\n",
    "HumanMessage(content=\"请帮我介绍一下什么是机器学习\"), ]\n",
    "messages2 = [SystemMessage(content=\"你是一位乐于助人的智能小助手\"),\n",
    "HumanMessage(content=\"请帮我介绍一下什么是AIGC\"), ]\n",
    "messages3 = [SystemMessage(content=\"你是一位乐于助人的智能小助手\"),\n",
    "HumanMessage(content=\"请帮我介绍一下什么是大模型技术\"), ]\n",
    "messages = [messages1, messages2, messages3]\n",
    "# 调用batch\n",
    "response = chat_model.batch(messages)\n",
    "print(response)\n"
   ],
   "id": "c681ff1c03e2a2e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "举例4：同步和异步方法的调用",
   "id": "2909ad6fd9454257"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "体会一：",
   "id": "a929bc7aa09d00b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "def call_model():\n",
    "# 模拟同步API调用\n",
    "    print(\"开始调用模型...\")\n",
    "    time.sleep(5) # 模拟调用等待,单位：秒\n",
    "    print(\"模型调用完成。\")\n",
    "def perform_other_tasks():\n",
    "    # 模拟执行其他任务\n",
    "    for i in range(5):\n",
    "        print(f\"执行其他任务 {i + 1}\")\n",
    "        time.sleep(1) # 单位：秒\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    call_model()\n",
    "    perform_other_tasks()\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    return f\"总共耗时：{total_time}秒\"\n",
    "# 运行同步任务并打印完成时间\n",
    "main_time = main()\n",
    "print(main_time)"
   ],
   "id": "a38ae2b144d7075f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "异步调用\n",
    "异步调用，允许程序在等待某些操作完成时继续执行其他任务，而不是阻塞等待。这在处理I/O操作（如\n",
    "网络请求、文件读写等）时特别有用，可以显著提高程序的效率和响应性。\n",
    "举例：\n",
    "写法1：此写法适合Jupyter Notebook"
   ],
   "id": "6ee88cd44f6ec01c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import asyncio\n",
    "import time\n",
    "async def async_call(llm):\n",
    "    await asyncio.sleep(5) # 模拟异步操作\n",
    "    print(\"异步调用完成\")\n",
    "async def perform_other_tasks():\n",
    "    await asyncio.sleep(5) # 模拟异步操作\n",
    "    print(\"其他任务完成\")\n",
    "async def run_async_tasks():\n",
    "    start_time = time.time()\n",
    "    await asyncio.gather(\n",
    "        async_call(None), # 示例调用，使用None模拟LLM对象\n",
    "        perform_other_tasks()\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    return f\"总共耗时：{end_time - start_time}秒\"\n",
    "# # 正确运行异步任务的方式\n",
    "# if __name__ == \"__main__\":\n",
    "# # 使用 asyncio.run() 来启动异步程序\n",
    "# result = asyncio.run(run_async_tasks())\n",
    "# print(result)\n",
    "# 在 Jupyter 单元格中直接调用\n",
    "result = await run_async_tasks()\n",
    "print(result)"
   ],
   "id": "b6d70813e16b3cad",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
