{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. TextSplitter çš„ä½¿ç”¨\n",
    "1ã€ä½¿ç”¨ç»†èŠ‚\n",
    "TextSplitter ä½œä¸ºå„ç§å…·ä½“çš„æ–‡æ¡£æ‹†åˆ†å™¨çš„çˆ¶ç±»\n",
    "\n",
    "å†…éƒ¨å®šä¹‰äº†ä¸€äº›å¸¸ç”¨çš„å±æ€§:\n",
    "\n",
    "chunk_size: è¿”å›å—çš„æœ€å¤§å°ºå¯¸ï¼Œå•ä½æ˜¯å­—ç¬¦æ•°ã€‚é»˜è®¤å€¼ä¸º 4000 (ç”±é•¿åº¦å‡½æ•°æµ‹é‡)ã€‚\n",
    "\n",
    "chunk_overlap: ç›¸é‚»ä¸¤ä¸ªå—ä¹‹é—´çš„å­—ç¬¦é‡å æ•°ï¼Œé¿å…ä¿¡æ¯åœ¨è¾¹ç•Œå¤„è¢«åˆ‡æ–­è€Œä¸¢å¤±ã€‚é»˜è®¤å€¼ä¸º 200ï¼Œé€šå¸¸ä¼šè®¾ç½®ä¸º chunk_size çš„ 10% - 20%ã€‚\n",
    "\n",
    "length_function: ç”¨äºæµ‹é‡ç»™å®šå—å­—ç¬¦æ•°çš„å‡½æ•°ã€‚é»˜è®¤å€¼ä¸º len å‡½æ•°ã€‚len å‡½æ•°åœ¨ Python ä¸­æŒ‰ Unicode å­—ç¬¦è®¡æ•°ï¼Œæ‰€ä»¥ä¸€ä¸ªæ±‰å­—ã€ä¸€ä¸ªè‹±æ–‡å­—æ¯ã€ä¸€ä¸ªç¬¦å·éƒ½ç®—ä¸€ä¸ªå­—ç¬¦ã€‚\n",
    "\n",
    "keep_separator: æ˜¯å¦åœ¨å—ä¸­ä¿ç•™åˆ†éš”ç¬¦ã€‚é»˜è®¤å€¼ä¸º Falseã€‚\n",
    "\n",
    "add_start_index: å¦‚æœä¸º Trueï¼Œåˆ™åœ¨å…ƒæ•°æ®ä¸­åŒ…å«èµ·å§‹ç´¢å¼•ã€‚é»˜è®¤å€¼ä¸º Falseã€‚\n",
    "\n",
    "strip_whitespace: å¦‚æœä¸º Trueï¼Œåˆ™ä»æ¯ä¸ªæ–‡æ¡£çš„å¼€å§‹å’Œç»“æŸå¤„å»é™¤ç©ºç™½å­—ç¬¦ã€‚é»˜è®¤å€¼ä¸º Trueã€‚\n",
    "\n",
    "\n",
    "å†…éƒ¨å®šä¹‰çš„å¸¸ç”¨çš„æ–¹æ³•:\n",
    "\n",
    "æƒ…å†µ1: æŒ‰ç…§å­—ç¬¦ä¸²è¿›è¡Œæ‹†åˆ†:\n",
    "\n",
    "split_text(xxx): ä¼ å…¥çš„å‚æ•°ç±»å‹: å­—ç¬¦ä¸² ; è¿”å›å€¼å€¼çš„ç±»å‹: List[str]\n",
    "\n",
    "create_documents(xxx): ä¼ å…¥çš„å‚æ•°ç±»å‹: List[str] ; è¿”å›å€¼å€¼çš„ç±»å‹: List[Document]\n",
    "\n",
    "åº•å±‚è°ƒç”¨äº† split_text(xxx)\n",
    "\n",
    "æƒ…å†µ2: æŒ‰ç…§Documentå¯¹è±¡è¿›è¡Œæ‹†åˆ†:\n",
    "\n",
    "split_documents(xxx): ä¼ å…¥çš„å‚æ•°ç±»å‹: List[Document] ; è¿”å›å€¼å€¼çš„ç±»å‹: List[Document]ã€‚"
   ],
   "id": "93331b72bd081bd8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "f8839c7ca04efe15"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.ç¤ºä¾‹æ–‡æœ¬\n",
    "text = \"\"\"\n",
    "LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ï¼Œä½¿å¼€å‘è€…\n",
    "èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚\n",
    "\"\"\"\n",
    "\n",
    "# 3.å®šä¹‰å­—ç¬¦åˆ†å‰²å™¨\n",
    "splitter = CharacterTextSplitter(\n",
    "    chunk_size=50, # æ¯å—å¤§å°\n",
    "    chunk_overlap=5, # å—ä¸å—ä¹‹é—´çš„é‡å¤å­—ç¬¦æ•°\n",
    "    # length_function=len,\n",
    "    separator=\"\", # è®¾ç½®ä¸ºç©ºå­—ç¬¦ä¸²æ—¶, è¡¨ç¤ºç¦ç”¨åˆ†éš”ç¬¦ä¼˜å…ˆ\n",
    ")\n",
    "\n",
    "# 4.åˆ†å‰²æ–‡æœ¬\n",
    "texts = splitter.split_text(text)\n",
    "\n",
    "# 5.æ‰“å°ç»“æœ\n",
    "for i, chunk in enumerate(texts):\n",
    "    print(f\"ç¬¬{i+1}å—:é•¿åº¦:{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"*-\" * 50)"
   ],
   "id": "973a5f575a648f37",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹2: ä½“ä¼šseparator",
   "id": "cba7700fa57521d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.å®šä¹‰è¦åˆ†å‰²çš„æ–‡æœ¬\n",
    "text = \"\"\"è¿™æ˜¯ä¸€ä¸ªç¤ºä¾‹æ–‡æœ¬å•Šã€‚æˆ‘ä»¬å°†ä½¿ç”¨CharacterTextSplitterå°†å…¶åˆ†å‰²æˆå°å—ã€‚åˆ†å‰²åŸºäºå­—ç¬¦æ•°ã€‚\n",
    "# LangChain æ˜¯ä¸€ä¸ªç”¨äºå¼€å‘ç”±è¯­è¨€æ¨¡å‹ã€‚é©±åŠ¨çš„åº”ç”¨ç¨‹åºçš„æ¡†æ¶çš„ã€‚å®ƒæä¾›äº†ä¸€å¥—å·¥å…·å’ŒæŠ½è±¡ã€‚ä½¿å¼€\n",
    "å‘è€…èƒ½å¤Ÿæ›´å®¹æ˜“åœ°æ„å»ºå¤æ‚çš„åº”ç”¨ç¨‹åºã€‚\n",
    "\"\"\"\n",
    "\n",
    "# 3.å®šä¹‰åˆ†å‰²å™¨å®ä¾‹\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=30, # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°\n",
    "    chunk_overlap=5, # å—ä¹‹é—´çš„é‡å¤å­—ç¬¦æ•°\n",
    "    separator=\"ã€‚\", # æŒ‰å¥å·åˆ†å‰²\n",
    ")\n",
    "\n",
    "# 4.å¼€å§‹åˆ†å‰²\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 5.æ‰“å°æ•ˆæœ\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"ç¬¬{i + 1}å—:é•¿åº¦:{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"-*\" * 50)"
   ],
   "id": "fd693f9ce53a07c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹3: ç†Ÿæ‚‰ keep_separatorã€separator ä»¥åŠ chunk_overlap ä½•æ—¶ç”Ÿæ•ˆ",
   "id": "a6a8450575cf926a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 2.å®šä¹‰è¦åˆ†å‰²çš„æ–‡æœ¬\n",
    "text = \"è¿™æ˜¯ç¬¬ä¸€æ®µæ–‡æœ¬ã€‚è¿™æ˜¯ç¬¬äºŒæ®µå†…å®¹ã€‚æœ€åä¸€æ®µç»“æŸã€‚\"\n",
    "\n",
    "# 3.å®šä¹‰å­—ç¬¦åˆ†å‰²å™¨\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"ã€‚\",\n",
    "    chunk_size=20,\n",
    "    chunk_overlap=8,\n",
    "    keep_separator=True # chunkä¸­æ˜¯å¦ä¿ç•™åˆ‡å‰²ç¬¦\n",
    ")\n",
    "\n",
    "# 4.åˆ†å‰²æ–‡æœ¬\n",
    "chunks = text_splitter.split_text(text)\n",
    "\n",
    "# 5.æ‰“å°ç»“æœ\n",
    "for i,chunk in enumerate(chunks):\n",
    "    print(f\"ç¬¬{i + 1}å—:é•¿åº¦:{len(chunk)}\")\n",
    "    print(chunk)\n",
    "    print(\"*-\" * 50)"
   ],
   "id": "e398df279c09dda",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹1: ä½¿ç”¨split_text()æ–¹æ³•æ¼”ç¤º",
   "id": "20dccf7fdf68aed0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.å®šä¹‰RecursiveCharacterTextSplitteråˆ†å‰²å™¨å¯¹è±¡\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 3.å®šä¹‰æ‹†åˆ†çš„å†…å®¹\n",
    "text=\"LangChainæ¡†æ¶ç‰¹æ€§\\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\\nè®°å¿†ç®¡ç†åŠŸèƒ½\\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹:éœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚\"\n",
    "\n",
    "# 4.æ‹†åˆ†å™¨åˆ†å‰²\n",
    "paragraphs = text_splitter.split_text(text)\n",
    "\n",
    "for para in paragraphs:\n",
    "    print(para)\n",
    "    print('------')"
   ],
   "id": "29360b43d38788ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "åˆ—å­2:",
   "id": "53fe006319aa47df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.å®šä¹‰RecursiveCharacterTextSplitteråˆ†å‰²å™¨å¯¹è±¡\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 3.å®šä¹‰åˆ†å‰²çš„å†…å®¹\n",
    "# text=\"LangChainæ¡†æ¶ç‰¹æ€§\\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\\nè®°å¿†ç®¡ç†åŠŸèƒ½\\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹:éœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚\"\n",
    "\n",
    "list=[\n",
    "    \"LangChainæ¡†æ¶ç‰¹æ€§\\nå¤šæ¨¡å‹é›†æˆ(GPT/Claude)\\nè®°å¿†ç®¡ç†åŠŸèƒ½\\né“¾å¼è°ƒç”¨è®¾è®¡ã€‚æ–‡æ¡£åˆ†æåœºæ™¯ç¤ºä¾‹:éœ€è¦å¤„ç†PDF/Wordç­‰æ ¼å¼ã€‚\"\n",
    "]\n",
    "\n",
    "# 4.åˆ†å‰²å™¨åˆ†å‰²\n",
    "# create_documents(): å½¢å‚æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨, è¿”å›å€¼æ˜¯Documentçš„åˆ—è¡¨\n",
    "paragraphs = text_splitter.create_documents(list)\n",
    "\n",
    "for para in paragraphs:\n",
    "    print(para)\n",
    "    print('------')"
   ],
   "id": "f5d0d13f4cae4c35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹3: ä½¿ç”¨create_documents()æ–¹æ³•æ¼”ç¤ºï¼Œå°†æœ¬åœ°æ–‡ä»¶å†…å®¹åŠ è½½æˆå­—ç¬¦ä¸²ï¼Œè¿›è¡Œæ‹†åˆ†",
   "id": "d588c3fa653bc5bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.æ‰“å¼€txtæ–‡ä»¶\n",
    "with open(\"asset/load/08-ai.txt\", encoding = \"utf-8\") as f:\n",
    "    state_of_the_union = f.read() # è¿”å›çš„æ˜¯å­—ç¬¦ä¸²\n",
    "\n",
    "# 3.å®šä¹‰RecursiveCharacterTextSplitter (é€’å½’å­—ç¬¦åˆ†å‰²å™¨)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=20,\n",
    "    # chunk_overlap=0,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# 4.åˆ†å‰²æ–‡æœ¬\n",
    "# æ³¨æ„: create_documents() æœŸæœ›è¾“å…¥ä¸€ä¸ªå­—ç¬¦ä¸²åˆ—è¡¨, æ‰€ä»¥è¿™é‡Œç”¨ [[]] åµŒå¥—æ¥åŒ…è£…\n",
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "\n",
    "# 5.æ‰“å°åˆ†å‰²æ–‡æœ¬\n",
    "for text in texts:\n",
    "    print(f\" {text.page_content}\")\n",
    "    print('------')\n"
   ],
   "id": "8fb235467d497faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹4: ä½¿ç”¨split_documents()æ–¹æ³•æ¼”ç¤ºï¼Œåˆ©ç”¨PDFLoaderåŠ è½½æ–‡æ¡£ï¼Œå¯¹æ–‡æ¡£çš„å†…å®¹ç”¨é€’å½’åˆ‡å‰²å™¨åˆ‡å‰²",
   "id": "e95fe3ea2dfbcbbd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2.å®šä¹‰PyPDFLoaderåŠ è½½å™¨\n",
    "loader = PyPDFLoader(\"./asset/load/02-load.pdf\")\n",
    "\n",
    "# 3.åŠ è½½æ–‡æ¡£å¯¹è±¡\n",
    "docs = loader.load() # è¿”å›Documentå¯¹è±¡æ„æˆçš„list\n",
    "# print(f\"ç¬¬0é¡µ: \\n{docs[0]}\")\n",
    "\n",
    "# 4.å®šä¹‰åˆ‡å‰²å™¨\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,\n",
    "    # chunk_size=120,\n",
    "    chunk_overlap=0,\n",
    "    # chunk_overlap=100,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "# 5.å¯¹pdfå†…å®¹è¿›è¡Œåˆ‡å‰²å¾—åˆ°æ–‡æ¡£å¯¹è±¡\n",
    "paragraphs = text_splitter.split_documents(docs)\n",
    "# paragraphs = text_splitter.create_documents([text])\n",
    "for para in paragraphs:\n",
    "    print(para.page_content)\n",
    "    print('------')"
   ],
   "id": "57868ce381c64296",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 4ã€å…·ä½“çš„æ‹†åˆ†å™¨3: TokenTextSplitter / CharacterTextSplitter\n",
    "\n",
    "ä¸¾ä¾‹1: ä½¿ç”¨TokenTextSplitter"
   ],
   "id": "7ce12c24b12fadc2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "# 2.åˆå§‹åŒ– TokenTextSplitter\n",
    "text_splitter = TokenTextSplitter(\n",
    "    chunk_size=33, # æœ€å¤§ token æ•°ä¸º 32\n",
    "    chunk_overlap=0, # é‡å  token æ•°ä¸º 0\n",
    "    encoding_name=\"cl100k_base\", # ä½¿ç”¨ OpenAI çš„ç¼–ç å™¨, å°†æ–‡æœ¬è½¬æ¢ä¸º token åºåˆ—\n",
    ")\n",
    "\n",
    "# 3.å®šä¹‰æ–‡æœ¬\n",
    "text = \"äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚äººå·¥æ™ºèƒ½æ˜¯æŒ‡é€šè¿‡è®¡ç®—æœºç¨‹åºæ¨¡æ‹Ÿäººç±»æ™ºèƒ½çš„ä¸€é—¨ç§‘å­¦ã€‚è‡ª20ä¸–çºª50å¹´ä»£è¯ç”Ÿä»¥æ¥,äººå·¥æ™ºèƒ½ç»å†äº†å¤šæ¬¡èµ·ä¼ã€‚\"\n",
    "\n",
    "# 4.å¼€å§‹åˆ‡å‰²\n",
    "texts = text_splitter.split_text(text)\n",
    "\n",
    "# 5.æ‰“å°åˆ†å‰²ç»“æœ\n",
    "print(f\"åŸå§‹æ–‡æœ¬è¢«åˆ†å‰²æˆäº† {len(texts)} ä¸ªå—:\")\n",
    "for i, chunk in enumerate(texts):\n",
    "    print(f\"å— {i+1}: é•¿åº¦: {len(chunk)} å†…å®¹: {chunk}\")\n",
    "    print(\"*-\" * 50)"
   ],
   "id": "8cd9c2145be4e490",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "ä¸¾ä¾‹2: ä½¿ç”¨CharacterTextSplitter",
   "id": "c235834754d2a96"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1.å¯¼å…¥ç›¸å…³ä¾èµ–\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "import tiktoken # ç”¨äºè®¡ç®—Tokenæ•°é‡\n",
    "# 2.å®šä¹‰é€šè¿‡Tokenåˆ‡å‰²å™¨\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\", # ä½¿ç”¨ OpenAI çš„ç¼–ç å™¨\n",
    "    chunk_size=18,\n",
    "    chunk_overlap=0,\n",
    "    separator=\"ã€‚\", # æŒ‡å®šä¸­æ–‡å¥å·ä¸ºåˆ†éš”ç¬¦\n",
    "    keep_separator=False, # chunkä¸­æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦\n",
    ")\n",
    "#3.å®šä¹‰æ–‡æœ¬\n",
    "text = \"äººå·¥æ™ºèƒ½æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€å‘æ¡†æ¶ã€‚å®ƒæ”¯æŒå¤šç§è¯­è¨€æ¨¡å‹å’Œå·¥å…·é“¾ã€‚ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæƒ³å‡ºå»è¸é’ã€‚ä½†æ˜¯åˆæ¯”è¾ƒæ‡’ä¸æƒ³å‡ºå»ï¼Œæ€ä¹ˆåŠ\"\n",
    "# 4.å¼€å§‹åˆ‡å‰²\n",
    "texts = text_splitter.split_text(text)\n",
    "print(f\"åˆ†å‰²åçš„å—æ•°: {len(texts)}\")\n",
    "\n",
    "# 5.åˆå§‹åŒ–tiktokenç¼–ç å™¨ï¼ˆç”¨äºTokenè®¡æ•°ï¼‰\n",
    "encoder = tiktoken.get_encoding(\"cl100k_base\") # ç¡®ä¿ä¸CharacterTextSplitterçš„encoding_nameä¸€è‡´\n",
    "# 6.æ‰“å°æ¯ä¸ªå—çš„Tokenæ•°å’Œå†…å®¹\n",
    "for i, chunk in enumerate(texts):\n",
    "    tokens = encoder.encode(chunk) # ç°åœ¨encoderå·²å®šä¹‰\n",
    "    print(f\"å— {i + 1}: {len(tokens)} Token\\nå†…å®¹: {chunk}\\n\")"
   ],
   "id": "e2c86730d973115b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 5ã€å…·ä½“çš„æ‹†åˆ†å™¨4: SemanticChunker: è¯­ä¹‰åˆ†å—",
   "id": "1a4bea9998128353"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# åŠ è½½æ–‡æœ¬\n",
    "with open(\"asset/load/09-ai1.txt\", encoding=\"utf-8\") as f:\n",
    "    state_of_the_union = f.read() # è¿”å›å­—ç¬¦ä¸²\n",
    "\n",
    "# è·å–åµŒå…¥æ¨¡å‹\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"OPENAI_API_KEY_T1\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"OPENAI_BASE_URL\")\n",
    "embed_model = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# è·å–åˆ‡å‰²å™¨\n",
    "text_splitter = SemanticChunker(\n",
    "    embeddings=embed_model,\n",
    "    breakpoint_threshold_type=\"percentile\",#æ–­ç‚¹é˜ˆå€¼ç±»å‹: å­—æ®µå€¼[\"ç™¾åˆ†ä½æ•°\",\"æ ‡å‡†å·®\",\"å››åˆ†ä½è·\",\"æ¢¯åº¦\"]é€‰å…¶ä¸€\n",
    "    breakpoint_threshold_amount=65.0 # æ–­ç‚¹é˜ˆå€¼æ•°é‡(æä½é˜ˆå€¼ -> é«˜åˆ†å‰²æ•æ„Ÿåº¦)\n",
    ")\n",
    "\n",
    "# åˆ‡åˆ†æ–‡æ¡£\n",
    "docs = text_splitter.create_documents(texts = [state_of_the_union])\n",
    "print(len(docs))\n",
    "for doc in docs:\n",
    "    print(f\"ğŸ” æ–‡æ¡£ {doc}:\")"
   ],
   "id": "b6c04e6058e79bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "15c23496c23e267c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
